{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/e9/c2b4c823b3959d475a570c1bd2df4125478e2e37b96fb967a87933ae7134/transformers-4.18.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers) (1.12.1)\n",
      "Collecting sentencepiece (from sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/87/b37ebc960d0a85e10785a1a92d796edbd975840bee150a9ae3ba5d7a0250/sentencepiece-0.1.99.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg (from sentence-transformers) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.6/site-packages (from sentence-transformers) (4.11.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /opt/conda/lib/python3.6/site-packages (from sentence-transformers) (3.2.5)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/54/81b1c3c574a1ffde54b0c82ed2a37d81395709cdd5f50e59970aeed5d95e/torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/df/1b454741459f6ce75f86534bdad42ca17291b14a83066695f7d2c676e16c/huggingface_hub-0.4.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sentence-transformers) (0.19.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/57/da0cb8e40437f88630769164a66afec8af294ff686661497b6c88bc08556/tokenizers-0.12.1.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting filelock (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/84/ce/8916d10ef537f3f3b046843255f9799504aa41862bfa87844b9bdc5361cd/filelock-3.4.1-py3-none-any.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/a0/a1/b153a0a4caf7a7e3f15c2cd56c7702e2cf3d89b1b359d1f1c5e59d68f4ce/importlib_metadata-4.8.3-py3-none-any.whl\n",
      "Collecting pyyaml>=5.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/85/79b9e5b4e8d3c0ac657f4e8617713cca8408f6cdc65d2ee6554217cedff1/PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Collecting packaging>=20.0 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/8e/8de486cbd03baba4deef4142bd643a3e7bbe954a784dc1bb17142572d127/packaging-21.3-py3-none-any.whl\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/fb/2eee67ebd59417a0a329ae5ae88b6a1bded20e66693c1851adf9cfcb065a/regex-2023.5.5-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.18.4)\n",
      "Collecting dataclasses; python_version < \"3.7\" (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting sacremoses (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->sentence-transformers) (5.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from torchvision->sentence-transformers) (1.11.0)\n",
      "Collecting typing-extensions (from torch>=1.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/df/d4a4974a3e3957fd1c1fa3082366d7fff6e428ddb55f074bf64876f8e8ad/zipp-3.6.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.7)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (0.11)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (6.7)\n",
      "Building wheels for collected packages: sentencepiece, tokenizers\n",
      "  Running setup.py bdist_wheel for sentencepiece ... \u001b[?25lerror\n",
      "  Complete output from command /opt/conda/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-63beoths/sentencepiece/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-60lbi8ls --python-tag cp36:\n",
      "  /opt/conda/lib/python3.6/distutils/dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'\n",
      "    warnings.warn(msg)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.6\n",
      "  creating build/lib.linux-x86_64-3.6/sentencepiece\n",
      "  copying src/sentencepiece/__init__.py -> build/lib.linux-x86_64-3.6/sentencepiece\n",
      "  copying src/sentencepiece/_version.py -> build/lib.linux-x86_64-3.6/sentencepiece\n",
      "  copying src/sentencepiece/sentencepiece_model_pb2.py -> build/lib.linux-x86_64-3.6/sentencepiece\n",
      "  copying src/sentencepiece/sentencepiece_pb2.py -> build/lib.linux-x86_64-3.6/sentencepiece\n",
      "  running build_ext\n",
      "  Package sentencepiece was not found in the pkg-config search path.\n",
      "  Perhaps you should add the directory containing `sentencepiece.pc'\n",
      "  to the PKG_CONFIG_PATH environment variable\n",
      "  No package 'sentencepiece' found\n",
      "  CMake Error: The source directory \"/tmp/pip-install-63beoths/sentencepiece/build\" does not appear to contain CMakeLists.txt.\n",
      "  Specify --help for usage, or press the help button on the CMake GUI.\n",
      "  Unknown argument --parallel\n",
      "  Unknown argument 16\n",
      "  Usage: cmake --build <dir> [options] [-- [native-options]]\n",
      "  Options:\n",
      "    <dir>          = Project binary directory to be built.\n",
      "    --target <tgt> = Build <tgt> instead of default targets.\n",
      "    --config <cfg> = For multi-configuration tools, choose <cfg>.\n",
      "    --clean-first  = Build target 'clean' first, then build.\n",
      "                     (To clean only, use --target 'clean'.)\n",
      "    --use-stderr   = Ignored.  Behavior is default in CMake >= 3.0.\n",
      "    --             = Pass remaining options to the native tool.\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-63beoths/sentencepiece/setup.py\", line 198, in <module>\n",
      "      test_suite='sentencepiece_test.suite',\n",
      "    File \"/opt/conda/lib/python3.6/site-packages/setuptools/__init__.py\", line 129, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/opt/conda/lib/python3.6/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/opt/conda/lib/python3.6/distutils/dist.py\", line 955, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/opt/conda/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/opt/conda/lib/python3.6/site-packages/wheel/bdist_wheel.py\", line 204, in run\n",
      "      self.run_command('build')\n",
      "    File \"/opt/conda/lib/python3.6/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/opt/conda/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/opt/conda/lib/python3.6/distutils/command/build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"/opt/conda/lib/python3.6/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/opt/conda/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/opt/conda/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 75, in run\n",
      "      _build_ext.run(self)\n",
      "    File \"/opt/conda/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py\", line 186, in run\n",
      "      _build_ext.build_ext.run(self)\n",
      "    File \"/opt/conda/lib/python3.6/distutils/command/build_ext.py\", line 339, in run\n",
      "      self.build_extensions()\n",
      "    File \"/opt/conda/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py\", line 194, in build_extensions\n",
      "      self.build_extension(ext)\n",
      "    File \"/tmp/pip-install-63beoths/sentencepiece/setup.py\", line 87, in build_extension\n",
      "      subprocess.check_call(['./build_bundled.sh', __version__])\n",
      "    File \"/opt/conda/lib/python3.6/subprocess.py\", line 291, in check_call\n",
      "      raise CalledProcessError(retcode, cmd)\n",
      "  subprocess.CalledProcessError: Command '['./build_bundled.sh', '0.1.99']' returned non-zero exit status 1.\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for sentencepiece\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for sentencepiece\n",
      "  Running setup.py bdist_wheel for tokenizers ... \u001b[?25lerror\n",
      "  Complete output from command /opt/conda/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-63beoths/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-gxd2200l --python-tag cp36:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.6\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for tokenizers\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for tokenizers\n",
      "  Complete output from command /opt/conda/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-63beoths/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" clean --all:\n",
      "  running clean\n",
      "  removing 'build/lib.linux-x86_64-3.6' (and everything under it)\n",
      "  'build/bdist.linux-x86_64' does not exist -- can't clean it\n",
      "  'build/scripts-3.6' does not exist -- can't clean it\n",
      "  removing 'build'\n",
      "  running clean_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed cleaning build dir for tokenizers\u001b[0m\n",
      "Failed to build sentencepiece tokenizers\n",
      "\u001b[31mawscli 1.16.17 has requirement PyYAML<=3.13,>=3.10, but you'll have pyyaml 6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mtransformers 4.18.0 has requirement numpy>=1.17, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mtransformers 4.18.0 has requirement tqdm>=4.27, but you'll have tqdm 4.11.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tokenizers, pyyaml, filelock, typing-extensions, zipp, importlib-metadata, packaging, huggingface-hub, regex, dataclasses, sacremoses, transformers, sentencepiece, torch, sentence-transformers\n",
      "  Running setup.py install for tokenizers ... \u001b[?25lerror\n",
      "    Complete output from command /opt/conda/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-63beoths/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-72zcoqql/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    /tmp/pip-build-env-9_45rpi9/lib/python3.6/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      setuptools.SetuptoolsDeprecationWarning,\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.6\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers\n",
      "    copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "    copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "    copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "    copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "    copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "    copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "    copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "    creating build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "    copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "    copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "    copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "    copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "    copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "    copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "    copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "    copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "    copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "    copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "    running build_ext\n",
      "    running build_rust\n",
      "    error: can't find Rust compiler\n",
      "    \n",
      "    If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "    \n",
      "    To update pip, run:\n",
      "    \n",
      "        pip install --upgrade pip\n",
      "    \n",
      "    and then retry package installation.\n",
      "    \n",
      "    If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/opt/conda/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-63beoths/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-72zcoqql/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-63beoths/tokenizers/\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster.db')\n",
    "df = pd.read_sql_table(con=engine,table_name=\"disaster_tweet\")\n",
    "X = df['message']\n",
    "Y = df.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "        \n",
    "    clean_tokens = [w for w in clean_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf_transformer', TfidfTransformer()),\n",
    "    ('classifier',MultiOutputClassifier(RandomForestClassifier())),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.75, random_state=42)\n",
    "\n",
    "pipeline.fit(X_train,Y_train)\n",
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.34      0.44      4608\n",
      "          1       0.82      0.93      0.87     15053\n",
      "\n",
      "avg / total       0.77      0.79      0.77     19661\n",
      "\n",
      "request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93     16288\n",
      "          1       0.77      0.40      0.53      3373\n",
      "\n",
      "avg / total       0.87      0.88      0.86     19661\n",
      "\n",
      "offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     19569\n",
      "          1       0.00      0.00      0.00        92\n",
      "\n",
      "avg / total       0.99      1.00      0.99     19661\n",
      "\n",
      "aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.86      0.79     11514\n",
      "          1       0.73      0.55      0.63      8147\n",
      "\n",
      "avg / total       0.73      0.73      0.72     19661\n",
      "\n",
      "medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96     18105\n",
      "          1       0.59      0.06      0.11      1556\n",
      "\n",
      "avg / total       0.90      0.92      0.89     19661\n",
      "\n",
      "medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98     18670\n",
      "          1       0.77      0.05      0.10       991\n",
      "\n",
      "avg / total       0.94      0.95      0.93     19661\n",
      "\n",
      "search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99     19128\n",
      "          1       0.56      0.02      0.04       533\n",
      "\n",
      "avg / total       0.96      0.97      0.96     19661\n",
      "\n",
      "security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19305\n",
      "          1       0.00      0.00      0.00       356\n",
      "\n",
      "avg / total       0.96      0.98      0.97     19661\n",
      "\n",
      "military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98     19030\n",
      "          1       0.73      0.06      0.11       631\n",
      "\n",
      "avg / total       0.96      0.97      0.96     19661\n",
      "\n",
      "child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     19661\n",
      "\n",
      "avg / total       1.00      1.00      1.00     19661\n",
      "\n",
      "water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97     18410\n",
      "          1       0.87      0.25      0.39      1251\n",
      "\n",
      "avg / total       0.95      0.95      0.94     19661\n",
      "\n",
      "food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96     17485\n",
      "          1       0.81      0.49      0.61      2176\n",
      "\n",
      "avg / total       0.92      0.93      0.92     19661\n",
      "\n",
      "shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96     17943\n",
      "          1       0.82      0.29      0.43      1718\n",
      "\n",
      "avg / total       0.93      0.93      0.92     19661\n",
      "\n",
      "clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19358\n",
      "          1       0.58      0.04      0.07       303\n",
      "\n",
      "avg / total       0.98      0.98      0.98     19661\n",
      "\n",
      "money\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19209\n",
      "          1       1.00      0.01      0.02       452\n",
      "\n",
      "avg / total       0.98      0.98      0.97     19661\n",
      "\n",
      "missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19438\n",
      "          1       1.00      0.01      0.03       223\n",
      "\n",
      "avg / total       0.99      0.99      0.98     19661\n",
      "\n",
      "refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98     19014\n",
      "          1       0.60      0.03      0.05       647\n",
      "\n",
      "avg / total       0.96      0.97      0.95     19661\n",
      "\n",
      "death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     18779\n",
      "          1       0.72      0.14      0.24       882\n",
      "\n",
      "avg / total       0.95      0.96      0.95     19661\n",
      "\n",
      "other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93     17052\n",
      "          1       0.53      0.03      0.06      2609\n",
      "\n",
      "avg / total       0.82      0.87      0.81     19661\n",
      "\n",
      "infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97     18378\n",
      "          1       0.33      0.00      0.00      1283\n",
      "\n",
      "avg / total       0.90      0.93      0.90     19661\n",
      "\n",
      "transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     18767\n",
      "          1       0.71      0.03      0.06       894\n",
      "\n",
      "avg / total       0.94      0.96      0.94     19661\n",
      "\n",
      "buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98     18659\n",
      "          1       0.73      0.08      0.15      1002\n",
      "\n",
      "avg / total       0.94      0.95      0.93     19661\n",
      "\n",
      "electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19254\n",
      "          1       1.00      0.01      0.02       407\n",
      "\n",
      "avg / total       0.98      0.98      0.97     19661\n",
      "\n",
      "tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19543\n",
      "          1       0.00      0.00      0.00       118\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19661\n",
      "\n",
      "hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19457\n",
      "          1       0.00      0.00      0.00       204\n",
      "\n",
      "avg / total       0.98      0.99      0.98     19661\n",
      "\n",
      "shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     19574\n",
      "          1       0.00      0.00      0.00        87\n",
      "\n",
      "avg / total       0.99      1.00      0.99     19661\n",
      "\n",
      "aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19437\n",
      "          1       0.00      0.00      0.00       224\n",
      "\n",
      "avg / total       0.98      0.99      0.98     19661\n",
      "\n",
      "other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98     18774\n",
      "          1       0.20      0.00      0.00       887\n",
      "\n",
      "avg / total       0.92      0.95      0.93     19661\n",
      "\n",
      "weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.96      0.90     14104\n",
      "          1       0.85      0.58      0.69      5557\n",
      "\n",
      "avg / total       0.85      0.85      0.84     19661\n",
      "\n",
      "floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97     18023\n",
      "          1       0.90      0.24      0.37      1638\n",
      "\n",
      "avg / total       0.93      0.93      0.92     19661\n",
      "\n",
      "storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96     17786\n",
      "          1       0.78      0.28      0.41      1875\n",
      "\n",
      "avg / total       0.91      0.92      0.91     19661\n",
      "\n",
      "fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19454\n",
      "          1       0.33      0.00      0.01       207\n",
      "\n",
      "avg / total       0.98      0.99      0.98     19661\n",
      "\n",
      "earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98     17777\n",
      "          1       0.90      0.63      0.74      1884\n",
      "\n",
      "avg / total       0.96      0.96      0.95     19661\n",
      "\n",
      "cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19255\n",
      "          1       0.73      0.03      0.05       406\n",
      "\n",
      "avg / total       0.97      0.98      0.97     19661\n",
      "\n",
      "other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97     18620\n",
      "          1       0.50      0.03      0.05      1041\n",
      "\n",
      "avg / total       0.92      0.95      0.92     19661\n",
      "\n",
      "direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.97      0.91     15857\n",
      "          1       0.71      0.28      0.40      3804\n",
      "\n",
      "avg / total       0.82      0.84      0.81     19661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_category = list(df.iloc[:,3:].columns)\n",
    "for i in range(Y_test.shape[1]):\n",
    "    print(output_category[i])\n",
    "    print(classification_report(Y_test.T[i], Y_pred.T[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'count_vectorizer__ngram_range': ((1, 1), (1, 2)),\n",
    "    'classifier__estimator__n_estimators': [2,5,10,20],\n",
    "    'classifier__estimator__min_samples_split': [2,5,10,20,40]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf_transformer', TfidfTransformer()),\n",
    "    ('classifier',MultiOutputClassifier(RandomForestClassifier())),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function tokenize at 0x7f0d611b5bf8>, vocabulary=None)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Vectorizer(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, x, y=None, min_count, vector_size):\n",
    "#         self.min_count=min_count\n",
    "#         self.vector_size=vector_size\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         w2v = word2vec(sentences, min_count=self.min_count, size = self.vector_size)\n",
    "#         return pd.DataFrame(X_tagged)\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            try:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "                    return True\n",
    "            except:\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "class StartingNounExtractor(BaseEstimator, TransformerMixin):\n",
    "    def starting_noun(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            try:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "                    return True\n",
    "            except:\n",
    "                return False\n",
    "        return False\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_noun)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        ('starting_verb', StartingVerbExtractor()),\n",
    "        ('starting_noun', StartingNounExtractor()),\n",
    "    ])),\n",
    "\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier())),\n",
    "])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.75, random_state=42)\n",
    "\n",
    "pipeline.fit(X_train,Y_train)\n",
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.33      0.42      4608\n",
      "          1       0.82      0.93      0.87     15053\n",
      "\n",
      "avg / total       0.77      0.79      0.77     19661\n",
      "\n",
      "request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.98      0.93     16288\n",
      "          1       0.78      0.35      0.48      3373\n",
      "\n",
      "avg / total       0.86      0.87      0.85     19661\n",
      "\n",
      "offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     19569\n",
      "          1       0.00      0.00      0.00        92\n",
      "\n",
      "avg / total       0.99      1.00      0.99     19661\n",
      "\n",
      "aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.88      0.79     11514\n",
      "          1       0.76      0.52      0.61      8147\n",
      "\n",
      "avg / total       0.73      0.73      0.72     19661\n",
      "\n",
      "medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96     18105\n",
      "          1       0.55      0.04      0.08      1556\n",
      "\n",
      "avg / total       0.89      0.92      0.89     19661\n",
      "\n",
      "medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98     18670\n",
      "          1       0.73      0.06      0.10       991\n",
      "\n",
      "avg / total       0.94      0.95      0.93     19661\n",
      "\n",
      "search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99     19128\n",
      "          1       0.76      0.02      0.05       533\n",
      "\n",
      "avg / total       0.97      0.97      0.96     19661\n",
      "\n",
      "security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19305\n",
      "          1       0.00      0.00      0.00       356\n",
      "\n",
      "avg / total       0.96      0.98      0.97     19661\n",
      "\n",
      "military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98     19030\n",
      "          1       0.73      0.04      0.08       631\n",
      "\n",
      "avg / total       0.96      0.97      0.96     19661\n",
      "\n",
      "child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     19661\n",
      "\n",
      "avg / total       1.00      1.00      1.00     19661\n",
      "\n",
      "water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97     18410\n",
      "          1       0.86      0.17      0.29      1251\n",
      "\n",
      "avg / total       0.94      0.95      0.93     19661\n",
      "\n",
      "food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96     17485\n",
      "          1       0.82      0.39      0.53      2176\n",
      "\n",
      "avg / total       0.92      0.92      0.91     19661\n",
      "\n",
      "shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.97     17943\n",
      "          1       0.82      0.37      0.51      1718\n",
      "\n",
      "avg / total       0.93      0.94      0.93     19661\n",
      "\n",
      "clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19358\n",
      "          1       0.82      0.06      0.11       303\n",
      "\n",
      "avg / total       0.98      0.99      0.98     19661\n",
      "\n",
      "money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19209\n",
      "          1       0.76      0.03      0.06       452\n",
      "\n",
      "avg / total       0.97      0.98      0.97     19661\n",
      "\n",
      "missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19438\n",
      "          1       1.00      0.01      0.02       223\n",
      "\n",
      "avg / total       0.99      0.99      0.98     19661\n",
      "\n",
      "refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98     19014\n",
      "          1       0.50      0.01      0.02       647\n",
      "\n",
      "avg / total       0.95      0.97      0.95     19661\n",
      "\n",
      "death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     18779\n",
      "          1       0.79      0.13      0.23       882\n",
      "\n",
      "avg / total       0.95      0.96      0.95     19661\n",
      "\n",
      "other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.99      0.93     17052\n",
      "          1       0.42      0.03      0.06      2609\n",
      "\n",
      "avg / total       0.81      0.87      0.81     19661\n",
      "\n",
      "infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97     18378\n",
      "          1       0.08      0.00      0.00      1283\n",
      "\n",
      "avg / total       0.88      0.93      0.90     19661\n",
      "\n",
      "transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     18767\n",
      "          1       0.84      0.04      0.08       894\n",
      "\n",
      "avg / total       0.95      0.96      0.94     19661\n",
      "\n",
      "buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98     18659\n",
      "          1       0.72      0.11      0.19      1002\n",
      "\n",
      "avg / total       0.94      0.95      0.94     19661\n",
      "\n",
      "electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19254\n",
      "          1       1.00      0.00      0.01       407\n",
      "\n",
      "avg / total       0.98      0.98      0.97     19661\n",
      "\n",
      "tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19543\n",
      "          1       0.00      0.00      0.00       118\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19661\n",
      "\n",
      "hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19457\n",
      "          1       0.00      0.00      0.00       204\n",
      "\n",
      "avg / total       0.98      0.99      0.98     19661\n",
      "\n",
      "shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     19574\n",
      "          1       0.00      0.00      0.00        87\n",
      "\n",
      "avg / total       0.99      1.00      0.99     19661\n",
      "\n",
      "aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19437\n",
      "          1       0.00      0.00      0.00       224\n",
      "\n",
      "avg / total       0.98      0.99      0.98     19661\n",
      "\n",
      "other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98     18774\n",
      "          1       0.11      0.00      0.00       887\n",
      "\n",
      "avg / total       0.92      0.95      0.93     19661\n",
      "\n",
      "weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.96      0.90     14104\n",
      "          1       0.85      0.53      0.65      5557\n",
      "\n",
      "avg / total       0.84      0.84      0.83     19661\n",
      "\n",
      "floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97     18023\n",
      "          1       0.90      0.34      0.49      1638\n",
      "\n",
      "avg / total       0.94      0.94      0.93     19661\n",
      "\n",
      "storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96     17786\n",
      "          1       0.79      0.29      0.42      1875\n",
      "\n",
      "avg / total       0.92      0.92      0.91     19661\n",
      "\n",
      "fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     19454\n",
      "          1       1.00      0.01      0.02       207\n",
      "\n",
      "avg / total       0.99      0.99      0.98     19661\n",
      "\n",
      "earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97     17777\n",
      "          1       0.91      0.55      0.68      1884\n",
      "\n",
      "avg / total       0.95      0.95      0.95     19661\n",
      "\n",
      "cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     19255\n",
      "          1       0.67      0.03      0.06       406\n",
      "\n",
      "avg / total       0.97      0.98      0.97     19661\n",
      "\n",
      "other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97     18620\n",
      "          1       0.47      0.01      0.03      1041\n",
      "\n",
      "avg / total       0.92      0.95      0.92     19661\n",
      "\n",
      "direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.97      0.91     15857\n",
      "          1       0.72      0.28      0.40      3804\n",
      "\n",
      "avg / total       0.82      0.84      0.81     19661\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "output_category = list(df.iloc[:,3:].columns)\n",
    "for i in range(Y_test.shape[1]):\n",
    "    print(output_category[i])\n",
    "    print(classification_report(Y_test.T[i], Y_pred.T[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
